{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_random_forest.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLlqiD5Jz9hCFlhuhhlp1A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithisirpa/sentiment-analysis/blob/main/sentiment_analysis_random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis in Python using Scikit Learn, Random Forest Algorithm, and TF-IDF Vectorizer "
      ],
      "metadata": {
        "id": "fEVVSE-DY3sQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "3BQK6vbaZGly"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNa2JIOHYHdR"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting the dataset"
      ],
      "metadata": {
        "id": "34flFDjxZb3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"Tweets.csv\"\n",
        "\n",
        "airline_tweets = pd.read_csv(dataset_url)\n",
        "airline_tweets.head()"
      ],
      "metadata": {
        "id": "ycNNUpOzZbMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Changing the plot size from default"
      ],
      "metadata": {
        "id": "fb3nz34jeA12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_size = plt.rcParams[\"figure.figsize\"]\n",
        "print(plot_size[0])\n",
        "print(plot_size[1])\n",
        "\n",
        "plot_size[0] = 8\n",
        "plot_size[1] = 6\n",
        "plt.rcParams[\"figure.figsize\"] = plot_size"
      ],
      "metadata": {
        "id": "PRAAnozXda60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc739932-3410-4333-f628-3afca8b3f8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n",
            "4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis"
      ],
      "metadata": {
        "id": "dvfBr3B8uhVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pie chart to visualize the no of airlines"
      ],
      "metadata": {
        "id": "gx6ySVSxeiuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "airline_tweets.airline.value_counts().plot(kind = 'pie', autopct = '%1.0f%%')"
      ],
      "metadata": {
        "id": "_EKMRCwtenLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribution of sentiments across all tweets"
      ],
      "metadata": {
        "id": "9HxPMqOFe2_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "airline_tweets.airline_sentiment.value_counts().plot(kind = 'pie', autopct = '%1.0f%%', colors = ['red','yellow','green'])"
      ],
      "metadata": {
        "id": "dkJMwh0Ue_jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribution of sentiment for all airlines"
      ],
      "metadata": {
        "id": "QRnn28GNfWtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "airline_sentiment = airline_tweets.groupby(['airline','airline_sentiment']).airline_sentiment.count().unstack()\n",
        "airline_sentiment.plot(kind = 'bar')"
      ],
      "metadata": {
        "id": "GM69xjmqfWXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seaborn library to view average confidence level for **tweets**"
      ],
      "metadata": {
        "id": "_flubF1zf4Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.barplot(x='airline_sentiment',y='airline_sentiment_confidence', data = airline_tweets)"
      ],
      "metadata": {
        "id": "DjV_FQcSf4yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dividing the dataset into features and label sets\n",
        "Features contains tweets, label set contains the sentiment of the tweet that we have to predict. For this we can use iloc method of the pandas data"
      ],
      "metadata": {
        "id": "hfBLX8eBgcBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = airline_tweets.iloc[:,10].values\n",
        "labels = airline_tweets.iloc[:,1].values"
      ],
      "metadata": {
        "id": "85U-3gSWg_yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing on the Data - Data Cleaning "
      ],
      "metadata": {
        "id": "gn1ZVHKMhwkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for data cleaning we use regular expressions\n",
        "import re"
      ],
      "metadata": {
        "id": "bl_i1GW5h6PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_features = []\n",
        "for sentence in range(0,len(features)):\n",
        "  processed_feature = re.sub(r'\\W', ' ',str(features[sentence])) # to remove special characters\n",
        "\n",
        "  processed_feature = re.sub(r'\\s+[a-zA-Z]\\s+', ' ',processed_feature) # to remove all single charcters\n",
        "\n",
        "  processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ',processed_feature) # to remove single characters from the start\n",
        "\n",
        "  processed_feature = re.sub(r'\\s+', ' ',processed_feature,flags = re.I) # to substitute multiple spaces with single space\n",
        "\n",
        "  processed_feature = re.sub(r'^b\\s+', ' ',processed_feature) # to remove prefixed 'b'\n",
        "\n",
        "  processed_feature = processed_feature.lower() # to convert to lower case\n",
        "\n",
        "  processed_features.append(processed_feature)\n",
        "\n",
        "processed_features"
      ],
      "metadata": {
        "id": "6Osmv4cjiD77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting Text in Numerical form\n",
        "Statistical algos use maths to train models since we need numbers to solve we need to convert text to numbers.\n",
        "\n",
        "Approach used are : Bag of Words, TF-IDF\n",
        "\n",
        "Scikit Library has this TF-IDF Vectorizer class that can be used to convert text features into TF-IDF feature vectors."
      ],
      "metadata": {
        "id": "xfTEzd34kgDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ttEOrA6kfpW",
        "outputId": "4cb4d988-4121-40a3-8c17-e8b81d57eab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Vectorizer\n",
        "Here the max_features = 2500 which means it only uses the 2500 most frequently occuring words to create a bag of words feature vector\n",
        "\n",
        "Words that occur less frequently are not useful for classification\n",
        "\n",
        "max_df specifies that only use those words that occur in a minimum of 80% of documents\n",
        "\n",
        "min_df shows that include words that occur in atleast 7 documents"
      ],
      "metadata": {
        "id": "23l_lBqxm1eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=2500, min_df = 7, max_df = 0.8, stop_words=stopwords.words('english'))\n",
        "\n",
        "processed_features = vectorizer.fit_transform(processed_features).toarray()"
      ],
      "metadata": {
        "id": "pdFPVg_smz6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dividing the data into training and test sets\n",
        "\n",
        "here test_size = 0.2 means that 20% data will be used for testing the dataset and eremaining 80% for training the data "
      ],
      "metadata": {
        "id": "kmkHy4YwoXMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "puxMaLlIohlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test, y_train, y_test = train_test_split(processed_features, labels, test_size = 0.2,random_state=0)"
      ],
      "metadata": {
        "id": "xI8VV1Gioq-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the data using Random Forest\n",
        "\n",
        "Random forest owing its ability to its ability to act upon non normalized data\n",
        "\n",
        "sklearn.ensemble module has this RandomForestClassifier class that can be used to train. To do this we need to call the fit method to the RandomForestClassifier class and pass the training features and labels as parameters"
      ],
      "metadata": {
        "id": "K5IbL0NypSJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "text_classifier = RandomForestClassifier(n_estimators = 200, random_state = 0)\n",
        "text_classifier.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3WeDwr1pY-z",
        "outputId": "2d6eb34d-337f-4f25-dde9-208d28c1622b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=200, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Predictions on model and evaluating the model\n",
        "\n",
        "For this we use predict method from the RandomForestClassifier class"
      ],
      "metadata": {
        "id": "EZD7F5hFqYbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = text_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "wr_s_hrCqwsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To evaluate the performance of the machine learning model\n",
        "\n",
        "We can use classification metrics- confusion matrix, F1 measure, accuracy, etc\n",
        "\n",
        "To find values for this metrics we can use classification_report, confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "rbkHAGcEq_me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "QryZJHlvrdgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "print(accuracy_score(y_test,predictions))"
      ],
      "metadata": {
        "id": "tO9ZP71-rmhV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}